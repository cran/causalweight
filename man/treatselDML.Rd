% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/treatselDML.R
\name{treatselDML}
\alias{treatselDML}
\title{Binary or multiple  treatment effect evaluation with double machine learning under sample selection/outcome attrition}
\usage{
treatselDML(
  y,
  d,
  x,
  s,
  z = NULL,
  selected = 0,
  dtreat = 1,
  dcontrol = 0,
  trim = 0.01,
  MLmethod = "lasso",
  k = 3
)
}
\arguments{
\item{y}{Dependent variable, may contain missings.}

\item{d}{Treatment variable, must be discrete, must not contain missings.}

\item{x}{Covariates, must not contain missings.}

\item{s}{Selection indicator. Must be 1 if \code{y} is observed (non-missing) and 0 if \code{y} is not observed (missing).}

\item{z}{Optional instrumental variable(s) for selection \code{s}. If \code{NULL}, outcome selection based on observables (\code{x},\code{d}) - known as "missing at random" - is assumed. If \code{z} is defined, outcome selection based on unobservables - known as "non-ignorable missingness" - is assumed. Default is \code{NULL}.}

\item{selected}{Must be 1 if ATE is to be estimated for the selected population without missing outcomes. Must be 0 if the ATE is to be selected for the total population. Default is 0 (ATE for total population). This parameter is ignored if \code{z} is \code{NULL} (under MAR, the ATE in the total population is estimated).}

\item{dtreat}{Value of the treatment in the treatment group. Default is 1.}

\item{dcontrol}{Value of the treatment in the control group. Default is 0.}

\item{trim}{Trimming rule for discarding observations with (products of) propensity scores that are smaller than \code{trim} (to avoid too small denominators in weighting by the inverse of the propensity scores). If \code{selected} is 0 (ATE estimation for the total population), observations with products of the treatment and selection propensity scores that are smaller than \code{trim} are discarded.  If \code{selected} is 1 (ATE estimation for the subpopulation with observed outcomes), observations with treatment propensity scores smaller than \code{trim} are discarded. Default for \code{trim} is 0.01.}

\item{MLmethod}{Machine learning method for estimating the nuisance parameters based on the \code{SuperLearner} package. Must be either  \code{"lasso"} (default) for lasso estimation,  \code{"randomforest"} for random forests, \code{"xgboost"} for xg boosting,  \code{"svm"} for support vector machines, \code{"ensemble"} for using an ensemble algorithm based on all previously mentioned machine learners, or \code{"parametric"} for linear or logit regression.}

\item{k}{Number of folds in k-fold cross-fitting. Default is 3.}
}
\value{
A \code{treatDML} object contains eight components, \code{effect}, \code{se}, \code{pval}, \code{ntrimmed}, \code{meantreat}, \code{meancontrol}, \code{pstreat}, and \code{pscontrol}:

\code{effect}: estimate of the average treatment effect.

\code{se}: standard error of the effect.

\code{pval}: p-value of the effect estimate.

\code{ntrimmed}: number of discarded (trimmed) observations due to extreme propensity scores.

\code{meantreat}: Estimate of the mean potential outcome under treatment.

\code{meancontrol}: Estimate of the mean potential outcome under control.

\code{pstreat}: P-score estimates for treatment in treatment group.

\code{pscontrol}: P-score estimates for treatment in control group.
}
\description{
Average treatment effect (ATE) estimation for assessing the average effects of discrete (multiple or binary) treatments under sample selection/outcome attrition. Combines estimation based on Neyman-orthogonal score functions with double machine learning to control for confounders in a data-driven way.
}
\details{
Estimation of the causal effects of binary or multiple discrete treatments under conditional independence, assuming that confounders jointly affecting the treatment and the outcome can be controlled for by observed covariates, and sample selection/outcome attrition. The latter might either be related to observables, which implies a missing at random assumption, or in addition also to unobservables, if an instrument for sample selection is available. Estimation is based on Neyman-orthogonal score functions for potential outcomes in combination with double machine learning with cross-fitting, see Chernozhukov et al (2018). To this end, one part of the data is used for estimating the model parameters of the treatment and outcome equations based machine learning. The other part of the data is used for predicting the efficient score functions. The roles of the data parts are swapped (using k-fold cross-fitting) and the average treatment effect is estimated based on averaging the predicted efficient score functions in the total sample.  Standard errors are based on asymptotic approximations using the estimated variance of the (estimated) efficient score functions.
}
\examples{
# A little example with simulated data (2000 observations)
\dontrun{
n=2000                            # sample size
p=100                             # number of covariates
s=2                               # number of covariates that are confounders
sigma=matrix(c(1,0.5,0.5,1),2,2)
e=(2*rmvnorm(n,rep(0,2),sigma))
x=matrix(rnorm(n*p),ncol=p)       # covariate matrix
beta=c(rep(0.25,s), rep(0,p-s))   # coefficients determining degree of confounding
d=(x\%*\%beta+rnorm(n)>0)*1         # treatment equation
z=rnorm(n)
s=(x\%*\%beta+0.25*d+z+e[,1]>0)*1   # selection equation
y=x\%*\%beta+0.5*d+e[,2]            # outcome equation
 y[s==0]=0
# The true ATE is equal to 0.5
output=treatselDML(y,d,x,s,z)
cat("ATE: ",round(c(output$effect),3),", standard error: ",
    round(c(output$se),3), ", p-value: ",round(c(output$pval),3))
output$ntrimmed}
}
\references{
Bia, M., Huber, M., Laffers, L. (2020): "Double machine learning for sample selection models", working paper, University of Fribourg.

Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., Robins, J. (2018): "Double/debiased machine learning for treatment and structural parameters", The Econometrics Journal, 21, C1-C68.

van der Laan, M., Polley, E., Hubbard, A. (2007): "Super Learner", Statistical Applications in Genetics and Molecular Biology, 6.
}
